# 本地模型配置指南：Ollama 与 LM Studio

Jp-Linker 支持两类本地能力：

1. **图片识别**（顶栏「图片识别」下拉）：用 Tesseract 做 OCR，或改用 **本地 VLM** 直接从截图识别文字，适合截屏不全、错行导致语意混乱的场景。
2. **OCR 梳理**（顶栏「OCR 梳理」下拉）：在已有 OCR 文本基础上，用本地模型把错行、乱序整理成通顺日文。

**图片输入方式**：除 **Alt+Q 截屏**外，可用 **「粘贴图片」** 按钮或 **Cmd+Shift+V / Ctrl+Shift+V**，从剪贴板粘贴图片（如 Snipaste 截图后复制），与截屏走同一套识别与 API 分析流程。

**不同阶段用不同模型（LM Studio）**：顶栏可分别配置：
- **图片识别** 选「LM Studio VLM」时，需填写 **VLM 模型 ID**（如 olmOCR 2），仅用于截图→文字。
- **OCR 梳理** 或 **翻译** 选「LM Studio」时，需填写 **文字模型 ID**（如 Hunyuan-MT-7B），用于梳理与中→日翻译。
- 点击 **「检测模型」** 可从 LM Studio 拉取本地模型列表，在下拉框中直接选择，无需手输 ID。

**日文预览（中→日翻译）**：顶栏可选择 **「翻译：Gemini」** / **「翻译：LM Studio」** / **「翻译：Ollama」**。选 LM Studio 或 Ollama 时，无需 API Key 即可在「日文预览」中看到本地模型返回的日文译文。

支持 **Ollama** 和 **LM Studio**。下面按**步骤**从零配置；若你主要用 LM Studio 并希望**同时用不同模型做不同事**（如 VLM 做图片识别、另一模型做 OCR 梳理），请重点看 **「LM Studio 多模型 / 多用途」** 一节。

---

## 配置前先确认

- **Ollama**：同一时间一般只跑一个模型，在请求里用「模型名」区分；换任务时需在终端里 `ollama run 另一个模型` 或接受当前已加载的模型。
- **LM Studio**：同一个本地服务（默认 `http://localhost:1234`）可以：
  - **单模型**：只加载一个模型，图片识别和 OCR 梳理共用它（需是 VLM 才能做图片识别）。
  - **多模型 / JIT**：加载多个模型或开启「按需加载」，在 Jp-Linker 里**图片识别**填 VLM 的模型 ID、**OCR 梳理**填文字模型的模型 ID，即可各用各的模型（见下方 LM Studio 多模型小节）。

---

## 用 VLM 做图片识别（推荐截屏错行场景）

若截屏内容不全、容易错行，可改用**视觉语言模型（VLM）**直接「图片 → 文字」，由模型理解版面与语序，输出更有序的日文。

- **Ollama**：在顶栏选择「图片识别：Ollama VLM」，地址与模型名与下方「方式一」一致；模型需为**视觉模型**，例如：
  - **olmocr2:7b**（文档/截屏优化，约 6GB，推荐）
  - 或其它支持图片的模型（如 `llava`、`qwen2-vl` 等，以 Ollama 库中为准）
- **LM Studio**：选择「图片识别：LM Studio VLM」，地址与模型 ID 与下方「方式二」一致；需在 LM Studio 中加载**带 Vision 能力的模型**（如 olmOCR 2、Qwen2-VL 等）。

选好 VLM 后，截屏会先经本地 VLM 识别再显示；可再配合「OCR 梳理」做二次整理（或关闭梳理，仅用 VLM 输出）。

---

## 方式一：Ollama

### 1. 安装 Ollama

- 打开 [https://ollama.com](https://ollama.com)，下载并安装对应系统版本（macOS / Windows）。
- 安装完成后，菜单栏（或托盘）会出现 Ollama 图标，表示已在后台运行。

### 2. 拉取并运行模型

在终端执行（任选一个，推荐日文/多语表现好的）：

```bash
# 推荐：多语与日文较好（约 4.7GB）
ollama run qwen2.5:7b

# 或：体积小、速度快（约 2GB）
ollama run llama3.2:3b

# 或：质量与速度折中
ollama run gemma2:9b
```

第一次会下载模型，完成后会进入对话界面；**不用一直停留在这里**，关掉终端或按 Ctrl+C 退出对话即可，Ollama 会在后台继续运行，模型已加载。

### 3. 在 Jp-Linker 里配置

1. 启动 Jp-Linker：`npm run app` 或 `npm run start`。
2. 在顶栏找到「OCR 梳理」下拉框，选择 **「OCR 梳理：Ollama」**。
3. 填写：
   - **Ollama 地址**：保持默认 `http://localhost:11434`（本机未改过端口就不用改）。
   - **模型名**：填你在终端里用的名字，例如 `qwen2.5:7b`、`llama3.2:3b`、`gemma2:9b`，**必须和 `ollama run xxx` 里的 xxx 一致**。
4. 配置会自动保存，下次打开无需再填。

### 4. 自检

- 截一张图（Alt+Q），若「OCR 梳理」选的是 Ollama，会先出现「本地识别中…」，再出现「AI 梳理中…」。
- 若提示连接失败，请确认：
  - 菜单栏/托盘里有 Ollama 且处于运行状态；
  - 地址是 `http://localhost:11434`（若你改过 OLLAMA_HOST，这里要一致）；
  - 模型名与 `ollama run 模型名` 完全一致（区分大小写）。

---

## 方式二：LM Studio（一步步配置）

### 第一步：安装并打开 LM Studio

1. 打开浏览器，访问 [https://lmstudio.ai](https://lmstudio.ai)。
2. 下载与你系统对应的版本（macOS / Windows）。
3. 安装完成后启动 LM Studio，等待主界面完全加载。

### 第二步：下载模型

1. 在 LM Studio 左侧边栏点击 **「Discover」**（发现 / 探索）。
2. 在搜索框输入模型名，例如：
   - 用于 **OCR 梳理**（纯文字）：`Qwen2.5 7B`、`Llama`、`Gemma` 等；
   - 用于 **图片识别**（VLM）：`olmocr`、`Qwen2.5 VL` 等，需带 **Vision** 能力。
3. 在搜索结果中选一个 7B 及以下的模型（M3 Max 64G 可跑），点击 **「Download」** / **「下载」**，选择 GGUF 格式（如 Q4_K_M），等待下载完成。
4. 下载好的模型会出现在左侧 **「My Models」** 列表中。

### 第三步：加载模型并启动本地服务器

1. 在 **「My Models」** 里点击要用的模型，再点击 **「Load Model」** / **「加载模型」**，等待加载完成。
2. 加载完成后，在界面顶部或侧边找到 **「Local Server」** / **「本地服务器」**，点进去。
3. 在 Local Server 页面：
   - 确认服务器状态为 **已启动**（绿色或 “Running”；默认端口 **1234**）。
   - 若未启动，点击 **「Start Server」** / **「启动服务器」**。
4. **重要**：在页面上找到 **「Model」** 或 **「模型 ID」** 的显示（可能是一串如 `qwen2.5-7b-instruct` 或带斜杠的名字），**复制或记下完整字符串**，后面在 Jp-Linker 里要**一字不差**填写。

### 第四步：在 Jp-Linker 里填写地址和模型 ID

1. 启动 Jp-Linker：在项目目录执行 `npm run app` 或先 `npm run build` 再运行打包后的应用。
2. 在应用**顶栏**找到两个下拉框：
   - **「图片识别」**：若要用 LM Studio 的 VLM 做截图识别，选 **「图片识别：LM Studio VLM」**。
   - **「OCR 梳理」**：若要用 LM Studio 做文字梳理，选 **「OCR 梳理：LM Studio」**。
3. 在顶栏出现的输入框中填写：
   - **LM Studio 地址**：默认 `http://localhost:1234`（若你在 LM Studio 里改过端口，这里填相同地址，如 `http://localhost:8080`）。
   - **模型 ID**：粘贴第三步记下的 **Model / 模型 ID**（区分大小写、空格、横杠，必须与 LM Studio 当前加载的模型一致）。
4. 配置会自动保存到浏览器本地，下次打开无需重填。

### 第五步：自检

- **只做 OCR 梳理**：在 Jp-Linker 里用 **Alt+Q** 截一张图，选「图片识别：Tesseract」+「OCR 梳理：LM Studio」，应出现「本地识别中…」再「AI 梳理中…」。
- **用 VLM 做图片识别**：选「图片识别：LM Studio VLM」，截屏后应出现「本地识别中…」，随后左侧显示识别出的日文。
- 若报错「连接失败」或「LM Studio HTTP xxx」：
  - 确认 LM Studio 的 **Local Server 已启动**；
  - 确认 Jp-Linker 里填的地址和端口与 LM Studio 一致；
  - 确认 **模型 ID** 与 LM Studio 当前加载的模型名称完全一致（可在 LM Studio 的 Server / API 页面再次核对）。

---

## LM Studio 多模型 / 多用途（同一服务、不同模型）

若你希望**图片识别用 VLM、OCR 梳理用纯文字模型**，可以共用同一个 LM Studio 服务（同一地址、同一端口），通过**不同的模型 ID** 区分。

### 方式 A：多模型会话（推荐内存足够时）

1. 在 LM Studio 中下载并加载**两个**模型，例如：
   - 一个 **VLM**（如 olmOCR 2、Qwen2.5-VL-7B）用于图片识别；
   - 一个 **纯文字** 模型（如 Qwen2.5-7B）用于 OCR 梳理。
2. 在 **Playground** 中开启 **「Multi Model Session」** / **「多模型会话」**（具体名称以 LM Studio 当前版本为准），让两个模型同时加载在同一服务器上。
3. 在 Local Server 或 API 页面确认两个模型都有对应的 **模型 ID**。
4. 在 Jp-Linker 中：
   - **LM Studio 地址**只填一份：`http://localhost:1234`。
   - **图片识别：LM Studio VLM** 时，在「模型 ID」处填 **VLM 的模型 ID**。
   - **OCR 梳理：LM Studio** 时，在「模型 ID」处填 **文字模型的模型 ID**。
5. 这样截屏会走 VLM 识别，梳理会走文字模型，互不干扰。

**注意**：同时加载两个 7B 级模型对显存/内存要求较高，M3 Max 64G 可尝试；若内存紧张，用下面的方式 B。

### 方式 B：按需加载（JIT）

1. 在 LM Studio 的**设置**（如 `Cmd+,` 或 `Ctrl+,`）中，找到 **「Just-In-Time (JIT) model loading」** 或 **「按需加载模型」**，**开启**。
2. 只加载一个模型时，请求里带哪个模型 ID，LM Studio 会按需加载对应模型（可能先卸载当前模型再加载目标模型，会有几秒到十几秒延迟）。
3. 在 Jp-Linker 里同样：
   - 图片识别填 **VLM 的模型 ID**；
   - OCR 梳理填 **文字模型的模型 ID**；
   - 地址统一为 `http://localhost:1234`。
4. 第一次用「图片识别」会加载 VLM，第一次用「OCR 梳理」会加载文字模型；切换任务时可能会经历一次模型切换时间。

### 小结（LM Studio）

| 用途       | 顶栏选择                     | 模型 ID 填什么           |
|------------|------------------------------|---------------------------|
| 图片识别   | 图片识别：LM Studio VLM      | VLM 的模型 ID（如 olmOCR 2） |
| OCR 梳理   | OCR 梳理：LM Studio          | 文字模型的模型 ID（如 Qwen2.5-7B） |

地址始终填同一份（如 `http://localhost:1234`）；多模型或 JIT 时通过**模型 ID** 区分不同功能。

---

## 对比小结

| 项目       | Ollama                    | LM Studio                    |
|------------|---------------------------|------------------------------|
| 默认地址   | `http://localhost:11434`  | `http://localhost:1234`      |
| 模型名来源 | `ollama run 模型名` 里的名字 | Local Server 页里的 Model ID |
| 特点       | 命令行拉取、后台常驻      | 图形界面、可多模型/JIT 多用途 |

两种方式只需选一种配置即可；若你同时安装了 Ollama 和 LM Studio，在 Jp-Linker 里通过「OCR 梳理」或「图片识别」下拉框切换使用哪种即可。**图片识别**选 VLM 时，请确保对应填写的模型为视觉模型（如 olmOCR 2）。

---

## 第一次配置 LM Studio 清单（可打勾）

- [ ] 已安装并打开 LM Studio  
- [ ] 已在 Discover 中下载至少一个模型（做 OCR 梳理用文字模型，做图片识别用 VLM）  
- [ ] 已在 My Models 中 Load Model  
- [ ] 已打开 Local Server 并启动服务器（端口如 1234）  
- [ ] 已记下/复制「Model」或「模型 ID」  
- [ ] 已在 Jp-Linker 顶栏选择「图片识别：LM Studio VLM」或「OCR 梳理：LM Studio」  
- [ ] 已填写 LM Studio 地址（如 `http://localhost:1234`）和模型 ID（与 LM Studio 完全一致）  
- [ ] 已用 Alt+Q 截屏自检，无报错且能出现识别/梳理结果  

若某一步报错，请回到对应步骤检查地址、端口、模型 ID 是否一致，以及 LM Studio 的 Local Server 是否处于已启动状态。
